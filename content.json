{"meta":{"title":"QtTao's Blog","subtitle":"团结 紧张 严肃 活泼","description":null,"author":"Qitian Tao","url":"http://yoursite.com"},"pages":[{"title":"分类标签","date":"2017-02-02T15:35:52.000Z","updated":"2017-02-02T15:36:27.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"About Me","date":"2017-02-02T15:03:03.000Z","updated":"2017-02-02T15:10:37.000Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"陶啟添，男，来自广东佛山90后，听港乐，看动漫平日喜欢折腾代码，懂点Python和C/C++希望成为一名合格的全栈，通过技术改变生活"}],"posts":[{"title":"HTTPS 免费申请和续期","slug":"https","date":"2017-04-02T15:03:48.000Z","updated":"2017-04-03T02:36:22.000Z","comments":true,"path":"2017/04/02/https/","link":"","permalink":"http://yoursite.com/2017/04/02/https/","excerpt":"缘由让个人网站从 HTTP 升级为 HTTPS 是对于众多 web 开发者来说是一件很体现逼格的事情，尤其是能获取免费的 SSL 证书。不花钱一分钱，又能把逼装得漂亮，岂不美哉。 作为一个幼体后端开发工程师, 哥只是个在公司打酱油的角色，平日的工作就是写一些简单的 API 。有一天，需求来了，意思是给刚开发了三期的 API 加上 SSL 认证。天啊，幸福来得太突然了。但心想这样如此简单的 API 还需要 SSL 证书。哥没有立刻答应，而且也作不了主，所以机智地把锅甩给了技术主管，但结局是可想而知的。 Let’s Encrypt没错！写这篇博文的时候，SSL 证书已经安装成功了，但过程却是相当曲折。为了配置好证书花费了数小时，还把原本的工作给耽误了，这样才有了这篇工作记录。 要开启 HTTPS 认证，就必须要从证书授权机构中获取一个证书，证书有收费的也有免费的。Let’s Encrypt 就是其中一个提供免费证书的机构。","text":"缘由让个人网站从 HTTP 升级为 HTTPS 是对于众多 web 开发者来说是一件很体现逼格的事情，尤其是能获取免费的 SSL 证书。不花钱一分钱，又能把逼装得漂亮，岂不美哉。 作为一个幼体后端开发工程师, 哥只是个在公司打酱油的角色，平日的工作就是写一些简单的 API 。有一天，需求来了，意思是给刚开发了三期的 API 加上 SSL 认证。天啊，幸福来得太突然了。但心想这样如此简单的 API 还需要 SSL 证书。哥没有立刻答应，而且也作不了主，所以机智地把锅甩给了技术主管，但结局是可想而知的。 Let’s Encrypt没错！写这篇博文的时候，SSL 证书已经安装成功了，但过程却是相当曲折。为了配置好证书花费了数小时，还把原本的工作给耽误了，这样才有了这篇工作记录。 要开启 HTTPS 认证，就必须要从证书授权机构中获取一个证书，证书有收费的也有免费的。Let’s Encrypt 就是其中一个提供免费证书的机构。 CertbotCertbot 是 Let’s Encrypt 官方推荐的一套自动化获取证书的工具。「自动化」说得好听，实情并不是下载好，直接运行就能完成配置。单是安装 Certbot 客户端就遇到麻烦了。网上有一堆博文介绍如何安装 Certbot，比如 yum install certbot，又或者直接从 Github 中克隆 repo git clone https://github.com/certbot/certbot，但如果作为开发者的你，平时有使用 Python 的习惯的话，安装这个工具就更为方便了，因为 pip 一早就已经收录了这个包。只需在终端键入如下命令即可完成安装。 1sudo pip install certbot 解决了安装问题之后，接下就是获取 SSL 证书。网上很多配置教程都会使用 Certbot 的 webroot 模式来生成证书，但对于像 API 这些微型服务，根本就没有根目录，这时使用 webroot 模式就行不通了。最便捷的方式就是使用 Certbot 的 standalone 模式，无需指定网站目录，同时自动启用服务器的 443 服务端口来验证域名的归属。 1sudo certbot --standalone --email xxx@xxx.com.cn -d example.com -d www.example.com 出现「Congratulations!」说明服务器所需的证书已经生成好了，它们被放置在 /etc/letsencrypt/live 目录下对应域名的文件夹里。 Nginx 启用 HTTPS因为原来的服务没有配置证书，所以只负责监听 80 端口，接收请求。现在的需求是既要保留 HTTP，又要新增 HTTPS 认证。因此，新增的配置内容只需添加在该服务对应的 Nginx conf 文件的最后即可。配置的内容如下所示： 12345678910111213141516171819202122server &#123; listen 443 ssl; server_name example.com; ### 以下是重点 ### ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letscrypt/live/example.com/privkey.pem; ssl_dhparam /etc/nginx/ssl/dhparam.pem; access_log /var/log/nginx/log/example_access.log main; error_log /var/log/nginx/log/example_err.log; location / &#123; proxy_pass http://127.0.0.1:9876; proxy_set_header X-Real-IP $ remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_http_version 1.1; proxy_set_header Connection \"Keep-Alive\"; proxy_set_header Proxy-Connection \"Keep-Alive\"; &#125;&#125; 简单地说明一下上述配置内容的重点部分。ssl_certficate 和 ssl_certficate_key 对应路径下的文件就是此前通过 Certbot 客户端生成好的证书与密钥。至于 ssl_dhparam 属于可选项。默认情况下，Nginx 会使用 openssl 提供的 DHE 参数，其中包含一个较弱的 1024 位的交换密钥。为了增强 Nginx 的密钥，我们可以通过下面的命令生成一个 2048 位的密钥。 1sudo openssl dhparm -out /etc/nginx/ssl/dhparam.pem 2048 然后检查部署的服务器 443 端口是否开启。最后，重载 Nginx 配置文件和重启 Nginx 服务。 12sudo nginx -s reloadsudo service nginx restart 证书监控和更新Let’s Encrypt 提供的证书只有 90 天的有效期。借助 Let’s Monitor 可以监控所有域名下的 SSL 证书的有效期。当证书快要过期时，Let’s Monitor 会发出邮件提示，提醒你更新证书，非常方便。 至于证书更新，Certbot 客户端提供了一个更新命令：certbot renew。需要注意一点是刚才生成证书时候使用的是 standalone 模式，验证域名需要启用 443 端口，但是此时 Nginx 服务已经占用了该端口，所以更新证书前必须把 Nginx 服务关掉，才可以成功。这可能会对服务器上正在运行的其他服务有微小的影响。 1sudo certbot renew --pre-hook 'sudo service nginx stop' --post-hook 'sudo service nginx start' --dry-run dry-run 代表测试模式，用于检验更新过程是否成功，但不会真的去更新证书。出现了「Congratulations」的字样就说明模拟更新成功。 测试工具Qualys SSL Labs 提供全面的 SSL 安全性测试，填入你的域名，等待数分钟后就给你的 HTTPS 配置一个安全级别分数。 注：最后感谢 Let’s Encrypt 组织提供的免费证书，还有技术主管的支持。","categories":[{"name":"web","slug":"web","permalink":"http://yoursite.com/categories/web/"}],"tags":[]},{"title":"ETL 可视化工具调研","slug":"etl-tools","date":"2017-03-13T05:36:07.000Z","updated":"2017-03-13T06:24:53.000Z","comments":true,"path":"2017/03/13/etl-tools/","link":"","permalink":"http://yoursite.com/2017/03/13/etl-tools/","excerpt":"什么是ETLETL是 Extract，Transform，Load 的是缩写，分别代表数据的抽取，转换和装载，也就是平时很繁琐，但却是数据分析的基础清洗工作。数据抽取指的是将数据从原始数据库中读取出来，原始的数据库既可以是关系型数据库，如 MySQL，PostgreSQL 等，又可以是非关系型数据库（NoSQL），如 MongoDB，Cassandra 等；数据转换是指根据后续的分析需求预先制定的规则对抽取的数据进行计算和转换，最终将异构的数据统一起来；数据装载一般是指将转换好的数据导入到数据仓库中，如 Hive，Amazon Redshift 等大数据仓库工具。总的来说，ETL 任务需要考虑以下四个方面： 数据结构设计 数据质量 数据传输速度 数据安全 而一款优秀的 ETL 可视化工具需要考虑有以下这些方面： 对数据变更的支持 详细的日志 支持各种数据库 容错机制 通知支持 低延迟 可扩展性 准确性","text":"什么是ETLETL是 Extract，Transform，Load 的是缩写，分别代表数据的抽取，转换和装载，也就是平时很繁琐，但却是数据分析的基础清洗工作。数据抽取指的是将数据从原始数据库中读取出来，原始的数据库既可以是关系型数据库，如 MySQL，PostgreSQL 等，又可以是非关系型数据库（NoSQL），如 MongoDB，Cassandra 等；数据转换是指根据后续的分析需求预先制定的规则对抽取的数据进行计算和转换，最终将异构的数据统一起来；数据装载一般是指将转换好的数据导入到数据仓库中，如 Hive，Amazon Redshift 等大数据仓库工具。总的来说，ETL 任务需要考虑以下四个方面： 数据结构设计 数据质量 数据传输速度 数据安全 而一款优秀的 ETL 可视化工具需要考虑有以下这些方面： 对数据变更的支持 详细的日志 支持各种数据库 容错机制 通知支持 低延迟 可扩展性 准确性 为何选择可视化工具，而不是自定义脚本 一致性：脚本难以维护，而且还要花费时间对生产环境进行部署，整体效率不高； 可加快开发循环：可视化，还有实时检查的功能； 用户友好：对初级用户来说入门容易，操作简单； 桌面 ETL 工具Kettle 简介Kettle （Pentaho Data Integration）是一款领先的开源ETL应用系统。功能包括数据迁移，数据的导出导入，数据的清洗整合。Kettle 对初级使用者十分友好。每一个数据整理的步骤都有可视化的界面，可拖放式的流程设计方式，即不需要书写代码就能完成数据清洗的工作。此外，Kettle 还能支持多种数据库接口，有 MySQL, Hadoop Hive, Impala 等连接类型。 Kettle数据清洗系统主要由 Spoon，Pan，Kitchen 和 Carte 四个部件组成。其中 Spoon 是一个完整的数据转换图形界面，作业（Jobs）和数据转换（transformations）又是 Spoon 的基础，多个数据转换的连接在一起就能构成一个作业。利用 Pan 可以批量运行由 Spoon 设计的数据转换流程，但没有图形界面，利用 Kitchen 可以批量执行含有多个数据转换流程的作业。整个数据转移的作业的调度和执行可以通过 Carte 页面进行观察和监控。 所有创建的作业都会以 xml 文件保存下来，方便数据转换的重现。如果执行作业过程中遇到到异常情况，待异常处理完毕后，作业将在中断处继续执行。显然在 Spoon 界面上执行作业效率会高一些（借助 Spoon 的可拖放的流程设计模式以及内置调度器），不过也可以通过 Pan 和 Kitchen 来执行数据转换和作业，并通过 cron 来进行任务的调度。 Talend 简介Talend Open Studio 是一套成熟的开源数据集成产品，主要用于整合，清洗，分离数据。它具有丰富数据整合功能，包括基于 Eclipse 的图形化开发环境，可拖放的作业以及储存完备的元数据，方便数据的再利用。除此之外，还有一系列的高级 ETL 功能，如字符串处理，自动查找和异常处理。 Talend Open Studio 的数据清洗工作是以 projects，jobs 和 components 为基础的，将 components 连接在一起形成 jobs，多个 jobs 构成 project。但是界面中有大量的设置项需要调整，而且一个 job 又有多个 components 选项，对用户不太友好。 任务可以通过界面执行（效率较低），也可以通过命令行执行，所有任务将会由 Talend Scheduler 进行调度。若调整任务的内容，或者任务之间的依赖关系，只需在界面上手动调整即可，所以调整将会同步更新到代码当中，定制起来十分灵活。而对于异常的处理，Talend 会终止正在进行任务，等待修复，然后从头开始执行任务。比起 Kettle，Talend 在大数据处理，定制的灵活性更佳。 CloverETL 简介CloverETL 是一款基于 Java 的跨平台开源 ETL 数据管理软件。其中开源部分是 CloverETL Engine，这也是 CloverETL 的大脑部分。但 Desinger，Server 以及 Cluster 都是付费使用的。CloverETL 套件既支持单机版，也有基于命令行操作方式，甚至可以作为 Java 库被其他应用调用。类似 Kettle 和 Talend，CloverETL 也可以图形化来展示数据处理流程晨，用箭头表示数据从一个任务转移到另一个任务。 CloverETL 由以下三个部分组成： CloverETL Engine 是数据清洗的核心部分 CLoverETL Designer 是设计数据清洗流程以及执行转换任务的可视化工具 CloverETL Server 是工作流，任务调度和监控，用户管理和实时 ETL 的企业级管理平台 桌面 ETL 应用的优缺点比较 工具 优点 缺点 IBM Infosphere DataStage 以灵活性和丰富功能著称； 1.学习难度较大；2.需具备较大内存和较强的能力；3.实现起来比较浪费时间 InformaticaPowerCenter 1.容易学习；2.可跟踪执行记录；3.实时清洗 技术的价值被削弱 Oracle Data Integrator 与 Oracle 数据仓库高度集成 1.一般用作批量处理；2.只有解决 ETL 的功能 Kettle (open source) 1.直观的可视化界面；2.完善的日志记录功能；3.多线程执行，跨平台 1.定制性较差；2.工具更新较缓慢 Talend (open source) 1.直观的可视化界面；2.灵活定制；3.跨平台 1.任务失败只能重头执行；2.设置过于复杂，对用户不太友好 CloverETL (open source) 1.直观的可视化界面；2.跨平台 只有 Engine 开源，其他部件收费 Web ETL 工具DOMO相比桌面版本的 ETL 工具，基于浏览器的 ETL 工具在 ETL 方面的工具较为弱化，这些工具大多数是数据分析和呈现的平台，也可以称为商业决策的工具，同时也会搭配着一些简易的 ETL 功能。DOMO Magic ETL 就是一个很好的例子。Magic ETL 能够帮助你完成数据清洗，整合和转换的工作，而不需要熟悉SQL的语法。DOMO 支持多种数据库接口，也支持本地上传的文件，同时拥有清晰的可视化界面和可拖放的部件来完成数据的整合。将整个数据处理流程保存下来之后，只要更新数据源，就能获得新的数据集。 完成 ETL 工作之后，借助 DOMO Dashboard 可以构建多种图表，作分析之用，还具备在线讨论协作的功能。当然，ETL，数据分析计算和图表的呈现都是通过云服务器处理的，这肯定能减少自己服务器的负载，但在某些情况下可能意味着较慢的速度。 主要特征： 直接连接到任何数据源，简化分析流程 只需拖放任务和一些简单的设置就能完成整合，清洗和转换数据工作 便捷和轻松将数据进行共享和可视化 Panoply &amp; Stitch （Singer.io）Panoply.io 是一款基于 web 的数据管理云平台，致力于为分析师，数据科学家简化数据预处理的流程。其数据仓库和分析平台是给予 Amazon Redshift、Elastic Search 以及 Hadoop 搭建的，号称能在10分钟之内将原始数据转化为复杂的分析和数据洞察。 Panoply.io 支持的数据来源包括关系型数据，NoSQL 和本地文件。利用 Stitch 这个数据抽取工具，并与 Panoply.io 账号关联起来，完成从数据抽取到数据展示的分析流程。经试用发现，其 EL 工作是由 Stitch 工具完成，而 T 的功能则由 Panoply.io 的 SQL 查询编辑器实现，并且能够记录查询语句，整体上跟 HUE 的功能类似。比起 DOMO， 界面比较简陋，功能也较单一。 不同于 DOMO 的是它能够将数据库与一些热门的 BI 可视化工具， 如 Tableau，Looker 和 re:dash 等连接起来，方便数据分析师选择自己喜爱的工具进行分析和展示。 Matillion ETL for Amazon RedshiftMatillion ETL 搭建在 Amazon Redshift 的基于 web 的 ETL/ELT 工具。 亮点在于： 利用 Redshift 计算能力进行数据转换，达到实时反馈的水平 基于浏览器的界面，还有协作，版本控制，可视化功能 快速安装 Web ETL 应用的优缺点比较 工具 优点 缺点 DOMO 1.无学习难度，不需要熟悉 SQL 语法；2.图表制作；3.在线讨论写作 ETL 功能有限 Panoly.io &amp; Stitch 1.无学习难度；2.支持多个 BI 平台 1.ETL/ELT 功能太弱；2.难以重现 ETL 过程 Matillion 1.专业 ETL/ELT 平台；2.计算能力强，实时反馈 依赖 Redshift 参考链接： A Survey of ETL Tools OpenSource ETL tools ETL Tools - Top 10 ETL Tools Reviews ETL Tools Open Source DI Tool Comparison Domo vs. Tableau Why our ETL tool doesn’t do transformations Panoply.io documentation ETL vs. ELT Matillion Overview 注：本报告被技术主管认为毫无价值，发表于此，自我反省。","categories":[{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/categories/ETL/"}],"tags":[]},{"title":"正向代理与反向代理（转）","slug":"forward-reverse-proxies","date":"2017-02-26T12:19:28.000Z","updated":"2017-03-13T05:38:25.000Z","comments":true,"path":"2017/02/26/forward-reverse-proxies/","link":"","permalink":"http://yoursite.com/2017/02/26/forward-reverse-proxies/","excerpt":"计算机网络中，代理（proxy）可分为「正向代理」和「反向代理」，比如著名的翻墙软件 「Shadowsocks」就是一款正向代理软件，全世界前1000的高流量网站都在用的 Web 服务器 「Nginx」也作为反向代理服务器，那么两者之间究竟有什么区别？我尝试着用浅显易懂的例子把两个概念解释清楚。","text":"计算机网络中，代理（proxy）可分为「正向代理」和「反向代理」，比如著名的翻墙软件 「Shadowsocks」就是一款正向代理软件，全世界前1000的高流量网站都在用的 Web 服务器 「Nginx」也作为反向代理服务器，那么两者之间究竟有什么区别？我尝试着用浅显易懂的例子把两个概念解释清楚。 正向代理A同学在大众创业、万众创新的大时代背景下开启他的创业之路，目前他遇到的最大的一个问题就是启动资金，于是他决定去找马云爸爸借钱，可想而知，最后碰一鼻子灰回来了，情急之下，他想到一个办法，找关系开后门，经过一番消息打探，原来A同学的大学老师王老师是马云的同学，于是A同学找到王老师，托王老师帮忙去马云那借500万过来，当然最后事成了。不过马云并不知道这钱是A同学借的，马云是借给王老师的，最后由王老师转交给A同学。这里的王老师在这个过程中扮演了一个非常关键的角色，就是代理，也可以说是正向代理，王老师代替A同学办这件事，这个过程中，真正借钱的人是谁，马云是不知道的，这点非常关键。 我们常说的代理也就是指正向代理。正向代理的过程，它隐藏了真实的请求客户端，服务端不知道真实的客户端是谁，客户端请求的服务都被代理服务器代替来请求，科学上网工具「 Shadowsocks 」扮演的就是典型的正向代理角色。在天朝访问 www.google.com 时会被无情的墙掉，要想翻越这堵墙，你可以在国外用 「Shadowsocks 」来搭建一台代理服务器，代理帮我们请求 www.google.com，代理再把请求响应结果返回给我。 反向代理大家都有过这样的经历，拨打10086 客服电话，一个地区的 10086 客服有几个或者几十个，你永远都不需要关心在电话那头的是哪一个，叫什么，男的，还是女的，漂亮的还是帅气的，你都不关心，你关心的是你的问题能不能得到专业的解答，你只需要拨通了10086的总机号码，电话那头总会有人会回答你，只是有时慢有时快而已。那么这里的 10086 总机号码就是我们说的反向代理。客户不知道真正提供服务的人是谁。 反向代理隐藏了真实的服务端，当我们访问 www.baidu.com 的时候，就像拨打 10086 一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，你不知道，也不需要知道，你只需要知道反向代理服务器是谁就好了，www.baidu.com 就是我们的反向代理服务器，反向代理服务器会帮我们把请求转发到提供真实计算的服务器那里去。「Nginx」就是性能非常好的反向代理服务器，它可以用来做负载均衡。 两者的区别在于代理的对象不一样，「正向代理」代理的对象是客户端，「反向代理」代理的对象是服务端。 注：该文转自《最通俗易懂地解释：正向代理与反向代理》","categories":[{"name":"网络","slug":"网络","permalink":"http://yoursite.com/categories/网络/"}],"tags":[]},{"title":"常用的hive日期函数","slug":"hive-date-functions","date":"2017-02-26T02:22:50.000Z","updated":"2017-02-26T02:44:41.000Z","comments":true,"path":"2017/02/26/hive-date-functions/","link":"","permalink":"http://yoursite.com/2017/02/26/hive-date-functions/","excerpt":"以下日期函数均基于hive 1.1.0-cdh5.7.1版本。 from_utc_timestamp：将UTC时间转换成指定时区参数类型：timestamp或者timestamp格式的string返回值类型：string 12SELECT FROM_UTC_TIMESTAMP(min_deal_time, 'GMT+8') FROM pubdate; -- min_deal_time TIMESTAMPSELECT FROM_UTC_TIMESTAMP('2016-11-10 16:59:56.0', 'GMT+8'); -- 2016-11-10 16:59:56.0 timstamp格式的string to_utc_timestamp：将某一时区下的时间转换成UTC时间参数类型：timestamp或者timestamp格式的string返回值类型：string 12SELECT TO_UTC_TIMESTAMP(min_comment_time, 'GMT+8') FROM pubdate; -- min_comment_time TIMESTAMPSELECT TO_UTC_TIMESTAMP('2016-11-10 16:59:56.0', 'GMT+8'); -- 2016-11-10 16:59:56.0 timstamp格式的string","text":"以下日期函数均基于hive 1.1.0-cdh5.7.1版本。 from_utc_timestamp：将UTC时间转换成指定时区参数类型：timestamp或者timestamp格式的string返回值类型：string 12SELECT FROM_UTC_TIMESTAMP(min_deal_time, 'GMT+8') FROM pubdate; -- min_deal_time TIMESTAMPSELECT FROM_UTC_TIMESTAMP('2016-11-10 16:59:56.0', 'GMT+8'); -- 2016-11-10 16:59:56.0 timstamp格式的string to_utc_timestamp：将某一时区下的时间转换成UTC时间参数类型：timestamp或者timestamp格式的string返回值类型：string 12SELECT TO_UTC_TIMESTAMP(min_comment_time, 'GMT+8') FROM pubdate; -- min_comment_time TIMESTAMPSELECT TO_UTC_TIMESTAMP('2016-11-10 16:59:56.0', 'GMT+8'); -- 2016-11-10 16:59:56.0 timstamp格式的string to_date：将日期timestamp格式的string或者timestamp转换成日期参数类型：timestamp或者timestamp格式的string返回值类型：string 12SELECT TO_DATE(start_time) FROM shop_promotion; -- min_comment_time TIMESTAMPSELECT TO_DATE('2016-11-02 10:10:00.0'); -- 返回 '2016-11-02' year：从日期中返回年份部分参数类型：string格式的date，timestamp格式的string, 或者timestamp格式的日期返回值类型：int 123SELECT YEAR(start_time) FROM shop_promotion LIMIT 100; -- start_time TIMESTAMPSELECT YEAR('2016-11-02 10:10:00.0'); -- 返回 2016SELECT YEAR('2016-12-12'); -- 返回 2016 month：从日期中返回月份部分参数类型：string格式的date，timestamp格式的string, 或者timestamp格式的日期返回值类型：int 123SELECT MONTH(end_time) FROM shop_promotion LIMIT 100; -- end_time TIMESTAMPSELECT MONTH('2016-11-02 10:10:00.0'); -- 返回 11SELECT MONTH('2016-12-12'); -- 返回 12 date：从字符串中返回日期部分参数类型：timestamp格式的时间，不接受timestamp格式的string返回值类型：string 1SELECT DATE(min_comment_time) FROM pubdate_origin; day：从日期中返回天的部分参数类型：string格式的date，timestamp格式的string, 或者timestamp格式的日期返回值类型：int 123SELECT DAY(end_time) FROM shop_promotion LIMIT 100;SELECT DAY('2016-11-02 10:10:00.0'); -- 返回 11SELECT DAY('2016-12-12'); -- 返回 12 date_sub：返回开始日期start_date减少days天后的日期参数类型：timestamp格式的时间或者string格式下的timestamp（date）| 整型返回值类型：string 123SELECT DATE_SUB(FROM_UTC_TIMESTAMP(min_deal_time, 'GTM+8'), 1)) FROM pubdate;SELECT DATE_SUB('2016-11-02 10:10:00.0', 1); -- 返回 2016-11-01SELECT DATE_SUB('2016-11-02', 2); -- 返回 2016-10-31 date_add：返回开始日期start_date增加days天后的日期参数类型：timestamp格式的时间或者string格式下的timestamp（date）| 整型返回值类型：string 123SELECT DATE_ADD(FROM_UTC_TIMESTAMP(min_deal_time, 'GTM+8'), 1)) FROM pubdate;SELECT DATE_ADD('2016-11-02 10:10:00.0', 1); -- 返回 2016-11-03SELECT DATE_ADD('2016-11-02', 2); -- 返回 2016-11-04 trunc：日期时间截断函数参数类型：timestamp或者string，还有自定义参数负责指定截断部分； SYYYY, YYYY, YEAR, SYEAR, YYY, YY, Y: 年份 Q: 季度 MONTH, MON, MM, RM: 月份 WW, W: Same day of the week as the first day of the month. DDD, DD, J: 转换成timestamp DAY, DY, D: 转换成当前日期所在星期的第一天的日期 HH, HH12, HH24: 转换成timestamp，包含（24小时制）小时 MI: 分钟返回值类型：string 123456-- 根据每周的第一天的日期和商品ID对每日销量进行汇总，求得商品的周销量SELECT product_id, SUM(count) AS weekly_deals, TO_DATE(TRUNC(dt, 'D')) AS weekFROM daily_deal_wideWHERE pubdate &gt;= '2016-10-11' AND pubdate &lt;= '2016-11-11' AND category_name = '裤子'GROUP BY week, product_idHAVING weekly_deals &gt;= 50","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"Python字符编码","slug":"python-character-coding","date":"2017-02-25T14:52:14.000Z","updated":"2017-02-26T02:51:03.000Z","comments":true,"path":"2017/02/25/python-character-coding/","link":"","permalink":"http://yoursite.com/2017/02/25/python-character-coding/","excerpt":"基本概念 字节：计算机数据存储的基本单元。比如1 Byte = 8 bits。 字符：信息单位，它是各种文字和符号的统称。 字符集：某个范围内字符的集合，比如ASCII有128个字符，GB2312有7445个字符。 字符码：字符集中每个字符的数字编号，比如ASCII有编号0至127。 字符编码：将字符集中的字符码映射为字节流的一种具体实现方案，比如ASCII字符编码规定使用单字节中低位的7bits去编码所有字符。「A -&gt; 65 -&gt; 0X41 -&gt; b’01000001’」 编码（encode）：将字符转换成字节流。 解码（decode）：将字节流解析为字符。","text":"基本概念 字节：计算机数据存储的基本单元。比如1 Byte = 8 bits。 字符：信息单位，它是各种文字和符号的统称。 字符集：某个范围内字符的集合，比如ASCII有128个字符，GB2312有7445个字符。 字符码：字符集中每个字符的数字编号，比如ASCII有编号0至127。 字符编码：将字符集中的字符码映射为字节流的一种具体实现方案，比如ASCII字符编码规定使用单字节中低位的7bits去编码所有字符。「A -&gt; 65 -&gt; 0X41 -&gt; b’01000001’」 编码（encode）：将字符转换成字节流。 解码（decode）：将字节流解析为字符。 ASCII码的历史英文字符只用一个字节的存储空间就能表示，比如1 Byte = 8 bits 能代表256个字符。ASCII字符集由96个文字和32个控制符组成，只需要7位就能表示所有字符，剩下最高位被用作一些通讯系统的奇偶校验。由于西欧字符的数量远远超出ACSII码所能表达的范围，扩展ASCII码（EASCII）应运而生，EASCII由原来的ASCII码和一些表格符号，计算符号，希腊字母和拉丁符号组成。具体的字符范围是0x00～0x7F + 0x80 ～0xFF。代表性的字符集有CP437和ISO/8859-1。 GBK多字节字符的编码GBF是为了解决汉字在计算机中的表达的问题。一般汉字使用双字节来编码的。代表性的字符集有GB312（共有6763个汉字）和GBK（共有27484个汉字）。 UnicodeUnicode是一种通用的编码方式，它是用十六进制数字表示的，一般在数字前面加上前缀U+，比如A的Unicode编码是「U+0041」，「中」的Unicode编码是「U+4E2D」。Unicode有两种格式：UCS-2和UCS-4。 UCS-2：使用两个字节，共16个比特，最多可以表达65536个字符。 UCS-4: 使用四个字节，共32个比特，最高位通常为0。理论上可以涵盖一切语言所用的符号。 Unicode是也有其局限性的。例如一个Unicode字符在网络上传输或者存储起来的时候，并不见得每个字符都需要两个字节，如字符A，统一使用两个字节会大大地浪费空间。另外，如何判断哪两个字节是表示一个字符，这需要不同的Unicode编码实现方法，如UTF-8和UTF-16。 UTF-8Unicode Transformation Format，简称UTF-8，是一种变长的字符编码实现方式，可以根据具体情况用1至4个字符来表示一个字符。比如英文字符这些原本就可以用ASCII码表示的字符用UTF-8表示时就只需要一个字节的空间，和ASCII是一样的。对于多字节的字符，第一个字节的前n位都设为1，第n+1位设为0，后面字节的前面两位都设为10。剩下的二进制位全部用该字符的Unicode码填充。以「陶」为例，「陶」的Unicode编码是「U+9676」，对应的UTF-8十六进制范围是00000800～0000FFFF，这表明了「陶」用UTF-8表示时需要用3个字节来存储，「U+9676」的二进制表示是1001011001110110，填充到1110xxxx 10xxxxxx 10xxxxxx得到11101001 10011001 10110110，转换成十六进制是e999b6。以下是Unicode和UTF-8转换关系表（x字符表示码点占据的位）。 起始值 终止值 Byte 1 Byte 2 Byte 3 Byte 4 Byte 5 Byte 6 0000 007F 0xxxxxxx 0080 007FF 110xxxxx 10xxxxxx 0800 FFFF 1110xxxx 10xxxxxx 10xxxxxx 10000 1FFFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 200000 3FFFFFF 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 4000000 7FFFFFFF 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx Python2编码Python2默认编码是ASCII，所以在Python2中源代码文件必须显示地指定编码类型，否则但凡代码中出现有中文就会报语法错误。Python2中的字符串有两种类型，分别是str和unicode，它们都是继承自basestring类，而str类型的字符串的编码格式可以是ascii，utf-8，gbk等任何一种类型。 str与unicode转换这两种类型的字符串之间的转换靠的就是decode和encode两个函数。encode负责将unicode编码成制定的字符编码，用语存储到磁盘或传输到网络中。而decode方法是根据指定编码方式解码后在应用程序中使用，所以从unicode转换到str用的是encode，从str类型转换到unicode则用decode。 UnicodeEncodeError或者UnicodeDecodeError的原因这些错误的根本原因在于Python2默认是使用ASCII编码进行decode和encode操作。 案例I 12s = '你好' # str --&gt; decodes = u'你好' # unicode --&gt; encode 案例II 当str类型与unicode类型的字符串混合使用时，str类型字符串会隐式地将str转换成unicode字符串。「python2默认会使用ascii编码来进行decode操作。」 案例Ⅲ 所有出现乱码的原因都可以归结为字符经过不同编码解码导致在编码过程种使用的编码格式不一致！比如utf-8编码的字符「陶」占用3个字节，解码成unicode后。如果再用gbk来解码后，只有两个字节长度了，最后出现了乱码问题，因此防止乱码的最好方式就是始终坚持使用同一种编码格式对字符进行编码和解码操作。 编码解码在Python中的实际应用encode()函数encode函数的声明：S.encode([encoding[,errors]]) -&gt; object，作用是将str或者unicode编码成str，其中encoding参数指定的是编码结果字符串的编码类型，如果encoding没有指定的话，那么编码类型取sys.getdefaultencoding()。123456789#!/usr/bin/env python# -*- coding: utf-8 -*-# 将unicode编码成str，指定返回的str对象编码是gb2312u = u'中国汉字's = u.encode('gbk')# 抛出UnicodeError错误，编码类型取sys.getdefaultcoding()，默认值为ascii，而ascii编码不支持中文u = u'中国汉字's = u.encode() decode()函数decode函数的声明：S.decode([encoding[.errors]]) -&gt; string or unicode，作用是将一个str对象解码成unicode对象，其中encoding参数设置是被解码字符串的解码类型，而返回unicode对象的默认编码是utf-8。12345678910111213141516171819202122232425262728#!/usr/bin/env python# -*- coding: utf-8 -*-# 抛出编码错误，写入文件时将unicode自动转换成str，然后str指定编码为ascii，但实际内容是以utf-8编码的汉字data = u'中国汉字'with open('file.txt', 'w') as fd: fd.write(data)# 解决方案1：手动编码unicodedata = u'中国汉字'with open('file.txt', 'w') as fd: fd.write(data.encode('utf-8'))# 解决方案2：更改python默认指定编码 (不推荐)import sysreload(sys)sys.setdefaultencoding('utf-8')data = u'中国汉字'with open('file.txt', 'w') as fd: fd.write(data.encode())# 解决方案3：将python指定编码设置为系统默认语系的编码 （不推荐）import sys, localec = locale.getdefaultlocale()reload(sys)sys.setdefaultencoding(c[1])data = u'中国汉字'with open('file.txt', 'w') as fd: fd.write(data.encode())","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[]},{"title":"生产者/消费者模式","slug":"producers-consumers","date":"2017-02-10T13:57:27.000Z","updated":"2017-02-10T14:27:54.000Z","comments":true,"path":"2017/02/10/producers-consumers/","link":"","permalink":"http://yoursite.com/2017/02/10/producers-consumers/","excerpt":"概念生产者消费者问题，也称有限缓冲问题（Bounded-buffer problem）是一个多线程同步问题的经典案例。该问题描述了两个共享固定大小缓冲区的线程——即所谓的“生产者”和“消费者”——在实际运行时会发生问题。生产者的主要作用是生成一定量的数据放到缓冲区中，然后重复此过程。与此同时，消费者也在缓冲区消耗这些数据。该问题的关键就是要保证生产者不会在缓冲区满时加入数据，消费者也不会在缓冲区中空时消耗数据。 生产者在缓冲区满时休眠，等到下次消费者消耗缓冲区中的数据的时候，生产者才能被唤醒，开始往缓冲区添加数据 消费者在缓冲区空时进入休眠，等到生产者往缓冲区添加数据之后，再唤醒消费者","text":"概念生产者消费者问题，也称有限缓冲问题（Bounded-buffer problem）是一个多线程同步问题的经典案例。该问题描述了两个共享固定大小缓冲区的线程——即所谓的“生产者”和“消费者”——在实际运行时会发生问题。生产者的主要作用是生成一定量的数据放到缓冲区中，然后重复此过程。与此同时，消费者也在缓冲区消耗这些数据。该问题的关键就是要保证生产者不会在缓冲区满时加入数据，消费者也不会在缓冲区中空时消耗数据。 生产者在缓冲区满时休眠，等到下次消费者消耗缓冲区中的数据的时候，生产者才能被唤醒，开始往缓冲区添加数据 消费者在缓冲区空时进入休眠，等到生产者往缓冲区添加数据之后，再唤醒消费者 1234# -*- coding: utf-8 -*from threading import Conditionimport timeimport random 123456789101112131415161718condition = Condition()class ConsumerThread(Thread): \"\"\" Consumer \"\"\" def run(self): global queue while True: condition.acquire() # 在消费前检查队列是否为空 if not queue: print \"Nothing in queue, consumer is waiting\" # 如果为空，调用condition实例的wait()方法 condition.wait() print \"Producer added something to queue and notified the consumer\" num = queue.pop(0) print \"Consumed\", num condition.release() time.sleep(random.random()) 1234567891011121314151617181920class ProducerThread(Thread): \"\"\" Producer \"\"\" def run(self): nums = range(5) global queue while True: condition.acquire() num = random.choice(nums) queue.append(num) print \"Produced\", num # 调用notify()方法后，consumer被唤醒，但唤醒不意味着它可以运行 condition.notify() # notify()并不是释放lock，调用notify()后，lock依然被生产者所持有 condition.release() time.sleep(random.random())if __name__ == '__main__': ProducerThread().start() ConsumerThread().start() 队列限制为队列增加大小限制，即生产者不能向一个满队列继续加入数据。 在加入数据前，生产者检查队列是否为满 如果不为满，生产者可以继续正常流程 如果为满，生产者必须等待，调用condition实例的wait() 消费者消耗对列，然后notify生产者 当消费者释放lock，消费可以acquire这个lock然后往队列中加入数据 12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding: utf-8 -*from threading import Thread, Conditionimport timeimport randomqueue = []MAX_NUM = 10condition = Condition()class ProducerThread(Thread): def run(self): nums = range(5) global queue while True: condition.acquire() if len(queue) == MAX_NUM: print \"Queue full, producer is waiting\" condition.wait() print \"Space in queue, Consumer notified the producer\" num = random.choice(nums) queue.append(num) print \"Produced\", num condition.notify() condition.release() time.sleep(random.random())class ConsumerThread(Thread): def run(self): global queue while True: condition.acquire() if not queue: print \"Nothing in queue, consumer is waiting\" condition.wait() print \"Producer added something to queue and notified the consumer\" num = queue.pop(0) print \"Consumed\", num condition.notify() condition.release() time.sleep(random.random())if __name__ == '__main__': ProducerThread().start() ConsumerThread().start() 特别地，Queue封装了Condition的行为，如wait()，notify()，acquire()。使用Queue可以方便的实现以上功能。注意get()和put()都有适当的notify()。 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*from threading import Threadimport timeimport randomfrom Queue import Queuequeue = Queue(10)class ProducerThread(Thread): def run(self): nums = range(5) global queue while True: num = random.choice(nums) # put()在插入数据前有一个获取lock的逻辑，同时，put()也会检查队列是否已满。如果已满，它会在内部调用wait()，生产者开始等待 queue.put(num) print \"Produced\", num time.sleep(random.random())class ConsumerThread(Thread): def run(self): global queue while True: # get()从队列中移出数据前会获取lock # get()会检查队列是否为空，如果为空，消费者进入等待状态 num = queue.get() # 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号，详见Python之多线程 # Queue.task_done()用于统计在队列中未完成的任务，这样Queue.join()就能获知任务的结束 queue.task_done() print \"Consumed\", num time.sleep(random.random())if __name__ == '__main__': ProducerThread().start() ConsumerThread().start()","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[]},{"title":"multiprocessing.Pool的进程锁","slug":"multiprocessing-pool-lock","date":"2017-02-04T09:16:51.000Z","updated":"2017-02-25T14:56:03.000Z","comments":true,"path":"2017/02/04/multiprocessing-pool-lock/","link":"","permalink":"http://yoursite.com/2017/02/04/multiprocessing-pool-lock/","excerpt":"通常情况下，python标准库中的multiprocessing.Lock对象是不可以作为进程池Pool的参数，这是因为进程锁不能通过pickle序列化。以下提供两种方法允许在多进程使用过程中引入进程锁，解决资源竞争问题。 参数绑定实例化Manager()和创建Manager.Lock()，但是使用Manager会产生大量的进程，同时所有获取锁和释放锁的请求都会发送给Manager，这样会造成进程资源的消耗。 1234567891011121314import multiprocessingfrom functools import partialdef main(): iterable = [1, 2, 3, 4, 5] pool = multiprocessing.Pool() m = multiprocessing.Manager() l = m.Lock() # functool.partial作用是把keywords，args的参数传入到func中后，生成一个新的函数，其实仍然是func函数，只是有部分参数已经代入 # 这里是为了解决pool.map函数只接受一个iterator作为参数，所以需要将iterator和进程锁绑定在一起 func = partial(target, l) pool.map(func, iterable) pool.close() pool.join()","text":"通常情况下，python标准库中的multiprocessing.Lock对象是不可以作为进程池Pool的参数，这是因为进程锁不能通过pickle序列化。以下提供两种方法允许在多进程使用过程中引入进程锁，解决资源竞争问题。 参数绑定实例化Manager()和创建Manager.Lock()，但是使用Manager会产生大量的进程，同时所有获取锁和释放锁的请求都会发送给Manager，这样会造成进程资源的消耗。 1234567891011121314import multiprocessingfrom functools import partialdef main(): iterable = [1, 2, 3, 4, 5] pool = multiprocessing.Pool() m = multiprocessing.Manager() l = m.Lock() # functool.partial作用是把keywords，args的参数传入到func中后，生成一个新的函数，其实仍然是func函数，只是有部分参数已经代入 # 这里是为了解决pool.map函数只接受一个iterator作为参数，所以需要将iterator和进程锁绑定在一起 func = partial(target, l) pool.map(func, iterable) pool.close() pool.join() 将进程锁置为全局变量1234567891011121314151617181920import multiprocessingdef target(iterable_item): for item in items: if some conditions: with lock: # write to stdout or logfile, etc passdef init(l): global lock lock = ldef main(): iterable = [1, 2, 3, 4, 5] l = multiprocessing.Lock() poll = multiprocessing.Pool(initializer=init, initargs=(l,)) pool.map(target, iterable) pool.close() pool.join()","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"多进程","slug":"多进程","permalink":"http://yoursite.com/tags/多进程/"}]},{"title":"堆排序","slug":"heapsort","date":"2017-02-02T15:55:13.000Z","updated":"2017-02-04T09:06:07.000Z","comments":true,"path":"2017/02/02/heapsort/","link":"","permalink":"http://yoursite.com/2017/02/02/heapsort/","excerpt":"堆堆的定义如下： 堆是一颗完全二叉树； 堆中的某个节点的值总是不大于或不小于其孩子节点的值； 堆中每个节点的子堆都是堆； 当父节点的键值总是大于或者等于任何一个子节点的键值时为最大堆。当父节点的键值总是小于或者等于任何一个子节点的键值时未最小堆。如下图所示，左边为最大堆，右边为最小堆。","text":"堆堆的定义如下： 堆是一颗完全二叉树； 堆中的某个节点的值总是不大于或不小于其孩子节点的值； 堆中每个节点的子堆都是堆； 当父节点的键值总是大于或者等于任何一个子节点的键值时为最大堆。当父节点的键值总是小于或者等于任何一个子节点的键值时未最小堆。如下图所示，左边为最大堆，右边为最小堆。 算法思想首先把有n个元素的数组a初始化创建为最大堆，然后循环执行如下过程直到数组为空为止： 把堆顶a[0]元素（为最大元素）和当前最大堆的最后一个元素交换； 最大堆元素个数减一； 由于第一步后根结点不再满足最大堆的定义，因此调整根结点使之满足最大堆的定义； 1234567891011121314151617181920212223242526void CreateHeap(int a[], int n, int h) &#123;/* 当完全二叉树中某个非叶结点a[h]（h = (n - 2)/2）的左孩子结点a[2h + 1]和右孩子结点a[2h + 2]都已是最大堆后，调整一个非叶结点a[h]使之满足最大堆。 */ int i, j, flag; int temp; /* i为要建堆的二叉树根结点下标 */ i = h; /* j为i的左孩子结点的下标 */ j = 2 * i + 1; temp = a[i]; flag = 0; /* 沿左右孩子中值较大者重复向下筛选 */ while (j &lt; n &amp;&amp; flag != 1) &#123; /* 寻找左右孩子结点中较大者， */ if (j &lt; n - 1 &amp;&amp; a[j] &lt; a[j + 1]) j++; if (temp &gt; a[i]) &#123; flag = 1; /* 否则把a[j]上移 */ &#125; else &#123; a[i] = a[j]; i = j; j = 2 * i + 1; &#125; &#125; a[i] = temp;&#125; 123456void InitCreateHeap(int a[], int n) &#123; /* 把数组元素a[0]~a[n - 1]初始化创建为最大堆 */ int i; for (i = (n - 2)/2; i &gt;= 0; i--) CreateHeap(a, n, i);&#125; 123456789101112131415void HeapSort(int a[], int n) &#123; int i; int temp; /* 初始化创建最大堆 */ InitCreateHeap(a, n); /* 当前最大堆个数每次递减一 */ for (i = n - 1; i &gt; 0; i--) &#123; temp = a[0]; a[0] = a[i]; a[i] = temp; /* 调整根结点满足最大堆 */ /* 子二叉树的根结点下标为零，结点个数为i */ CreateHeap(a, i, 0); &#125;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/数据结构/"}],"tags":[{"name":"排序","slug":"排序","permalink":"http://yoursite.com/tags/排序/"}]},{"title":"关于图的知识点","slug":"graph","date":"2017-02-01T15:18:10.000Z","updated":"2017-02-04T08:38:11.000Z","comments":true,"path":"2017/02/01/graph/","link":"","permalink":"http://yoursite.com/2017/02/01/graph/","excerpt":"图的存储结构 邻接矩阵（稠密矩阵：点少边多的情况） 邻接表 （稀疏矩阵：点多边少的情况） 图的遍历 深度优先 123456789101112131415void DepthFSearch(AdjMGraph G, int v, int visited[], void Visit(char item))&#123; int w; Visit(G.Vertices.list[v]); visited[v]; w = GetFirstVex(G, v); /* 寻找当前顶点的邻接顶点 */ while (w != -1) &#123; if (!visited[w]) DepthFSearch(G, w, visited, Visit) /* 若邻接顶点已被访问，则寻找下一个邻接顶点 */ w = GetNextVex(G, v, w); &#125;&#125;","text":"图的存储结构 邻接矩阵（稠密矩阵：点少边多的情况） 邻接表 （稀疏矩阵：点多边少的情况） 图的遍历 深度优先 123456789101112131415void DepthFSearch(AdjMGraph G, int v, int visited[], void Visit(char item))&#123; int w; Visit(G.Vertices.list[v]); visited[v]; w = GetFirstVex(G, v); /* 寻找当前顶点的邻接顶点 */ while (w != -1) &#123; if (!visited[w]) DepthFSearch(G, w, visited, Visit) /* 若邻接顶点已被访问，则寻找下一个邻接顶点 */ w = GetNextVex(G, v, w); &#125;&#125; 广度优先 1234567891011121314151617181920212223242526void BoardFSearch(AdjMGraph G, int v, int visited[], void Visit(char item))&#123; int u, w; SeqCQueue queue; QueueInitiate(&amp;queue); Visit(G.Vertices.list[v]); visited[v]; QueueAppend(&amp;queue, v); /* 当队列为空时终止循环 */ while(QueueNotEmpty(queue)) &#123; QueueDelete(&amp;queue, &amp;u); w = GetFirstVex(G, u); /* 遍历所有邻接顶点 */ while(w != -1) &#123; if (!visited[w]) &#123; Visit(G.Vertices.list[w]); // 访问顶点w visited[w]; // 标记顶点w QueueAppend(&amp;queue, w); // 顶点w入队列 &#125; w = GetNextVex(G, u, w); &#125; &#125;&#125; 最小生成树－无向带权连通图对于无向带权连通图，其所有生成树中边的权值总和最小的是最小生成树。例如在n个城市之间敷设光缆，且各城市之间敷设光缆的费用不同。 n个顶点的无向连通带权图的最小生成树必须满足如下条件： 包括n个顶点 有且只有n - 1条边 不存在回路 经典算法 Prim algorithm （点少边多，稠密矩阵） 1234567891011121314151617181920212223242526272829303132333435363738typedef struct&#123; Vert vertex; int weight;&#125; MinSpanTree;/* Prim algorithm */void Prim(AdjMGraph G, MinSpanTree closeVertex[])&#123; Vert x; int i, j, k; int minCost; int n = G.Vertices.size; int * lowCost = (int *)malloc(sizeof(int) * n); for (i = 1; i &lt; n; i ++) lowCost[i] = G.edge[0][i]; lowCost[0] = -1; listGet(G.Vertices, 0, &amp;x); closeVertex[0].vertex = x; for (i = 1; i &lt; n; i++) &#123; minCost = MaxWeight; // MaxWeight为定义的最大权值 /* 寻找当前最小权值的边所对应的顶点k，但不包括lowCost = -1的 */ for (j = 1; j &lt; n; j++) if (minCost &gt; lowCost[j] &amp;&amp; lowCost[j] &gt; 0) &#123; k = j; minCost = lowCost[j]; &#125; listGet(G.Vertices, k, &amp;x); closeVertex[i].vertex = x; closeVertex[i].weight = minCost; lowCost[k] = -1; /* 更新权值 */ for (j = 1; j &lt; n; j++) if (lowCost[j] &gt; G.edge[k][j]) lowCost = G.edge[k][j]; &#125;&#125; Kruskal算法（点多边少，稀疏矩阵）：首先是带权图中各边的权值排序，其次是判断新选取的边的两个顶点是否属于同一个连通分量。 最短路径－有向带权图（连通／非连通）经典算法 Dijkstra algorithm：按路径长度递增的顺序逐步产生最短路径的构造算法 1234567891011121314151617181920212223242526272829303132333435363738void Dijkstra(AdjMGraph G, int v0, int distance[], int path[])/* s[]用来表示顶点是否已从集合T移动到集合S中 *//* path[]存放了从原点v0到其它各个顶点的最短路径的前一个顶点的下标 */&#123; int n = G.Vertices.size; int * s = (int *)malloc(sizeof(int) * n); int minDis, i, j, k; /* Initilization */ for (i = 0; i &lt; n; i++) &#123; distance[i] = G.edge[v0][j]; s[i] = 0; if (i != v0 &amp;&amp; distance[i] &lt; MaxWeight) path[i] = v0; else path[i] = -1; &#125; s[v0] = 1; /* 对于连通图来说，每一个循环能找到一个路径最短的顶点 */ for (i = 1; i &lt; n; i++) &#123; minDis = MaxWeight; for (j = 0; j &lt; n; j++) &#123; if (s[i] == 0 &amp;&amp; minDis &gt; distance[j]) &#123; k = j; minDis = distance[j]; &#125; &#125; /* 当不再存在路径时，算法结束。此语句对非连通图是必需的 */ if (minDis == MaxWeight) return; s[k] = 1; /* 修改从v0到其它顶点的最短路径 */ for (j = 0; j &lt; n; j++) if (s[i] == 0 &amp;&amp; G.edge[k][j] &lt; MaxWeight &amp;&amp; distance[k] + G.edge[k][j] &lt; distance[j]) distance[j] = distance[k] + G.edge[k][j]; path[j] = k; &#125;&#125; Floyd algorithm：解决每对顶点之间最短路径的算法。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/数据结构/"}],"tags":[{"name":"图","slug":"图","permalink":"http://yoursite.com/tags/图/"}]},{"title":"快速排序","slug":"quicksort","date":"2017-02-01T15:07:16.000Z","updated":"2017-02-02T15:59:08.000Z","comments":true,"path":"2017/02/01/quicksort/","link":"","permalink":"http://yoursite.com/2017/02/01/quicksort/","excerpt":"快速排序（Quicksort) 在平均状况下，排序n个项目要O(nlogn)次比较。在最坏状况下则需要O(n2)次比较，但这种状况并不常见。它在排序效率同为O(nlogn)的几种排序方法中效率最高，因此经常被采用。 算法思想设数组a中存放了n个数据元素，low为数组的低端下标，high为数组的高端下标，从数组a中任取一个元素（通常取a[low]）作为标准，调整数组a中各个元素的位置，使排在标准元素前面的元素的关键字均小于标准元素的关键字，排在标准元素后面的元素的关键字均大于或等于标准元素的关键字。（即标准元素被放在了未来排好序的数组中该标准元素应在的位置）然后，对这两个字数组中的元素分别再进行方法类同的递归快速排序。递归算法的结束条件是high &lt;= low，即上界下表小于或等于下界下标。","text":"快速排序（Quicksort) 在平均状况下，排序n个项目要O(nlogn)次比较。在最坏状况下则需要O(n2)次比较，但这种状况并不常见。它在排序效率同为O(nlogn)的几种排序方法中效率最高，因此经常被采用。 算法思想设数组a中存放了n个数据元素，low为数组的低端下标，high为数组的高端下标，从数组a中任取一个元素（通常取a[low]）作为标准，调整数组a中各个元素的位置，使排在标准元素前面的元素的关键字均小于标准元素的关键字，排在标准元素后面的元素的关键字均大于或等于标准元素的关键字。（即标准元素被放在了未来排好序的数组中该标准元素应在的位置）然后，对这两个字数组中的元素分别再进行方法类同的递归快速排序。递归算法的结束条件是high &lt;= low，即上界下表小于或等于下界下标。 对快速排序 （挖坑填数）进行形象化总结： i = L; j = R; 将基准数（pivot）挖出形成第一个坑a[i]。 j– 由后向前找比它小的数，找到后挖出此数填入前一个坑a[i]中。 i++ 由前向后找比它大的数，找到后也挖出此数填入前一个坑a[j]中。 再重复之行2，3二步，直到i == j，将基准数（pivot）填入a[i]中。 递归调用上述步骤，直到数组中全部元素从小到大排序完成。 1234567891011121314151617181920212223242526void QuickSort(int a[], int low, int high)/* 用递归方法对数据元素a[low]~a[high]进行快速排序 */&#123; int i = low, j = high; int temp = a[low]; while (i &lt; j) &#123; while(i &lt; j &amp;&amp; temp &lt;= a[j]) j--; //从数组右端扫描，找比基准小的数并赋给a[i] if (i &lt; j) &#123; a[i] = a[j]; i++; &#125; while (i &lt; j &amp;&amp; a[i] &lt; temp) i++; //从数组左端扫描，找比基准大的数并赋给a[j] if (i &lt; j) &#123; a[j] = a[i]; j--; &#125; &#125; a[i] = temp; /* 最后i == j */ if (low &lt; i) QuickSort(a, low, i - 1); if (i &lt; high) QuickSort(a, j + 1, high);&#125; 排序算法性能比较","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/数据结构/"}],"tags":[{"name":"排序","slug":"排序","permalink":"http://yoursite.com/tags/排序/"}]},{"title":"hive与hbase的区别","slug":"hive-hbase","date":"2017-02-01T14:57:57.000Z","updated":"2017-02-04T12:36:36.000Z","comments":true,"path":"2017/02/01/hive-hbase/","link":"","permalink":"http://yoursite.com/2017/02/01/hive-hbase/","excerpt":"定义 hive是一个构建在hadoop基础设施上的数据仓库，通过hive可以使用hql语言查询存放在hdfs上的数据。hql是一种类sql语言，这种语言最终被转化为map/reduce。虽然hive提供了sql查询功能，但是hive不能进行交互查询。（只能在hadoop上批量执行hadoop） hbase是一种key/value系统，运行在hdfs之上。和hive不一样，hbase能够在数据库上实时运行，而不是运行map/reduce任务。hive被分区为表格，表格又被进一步分割为列簇。列簇必须使用schema定义，列簇将某一类型列集合起来。每一个key/value对在hbase中被定义为一个cell，每一个key由row-key，列簇，列和时间戳。在hbase中，行是key/value映射的集合，这个映射通过row-key来唯一标识。","text":"定义 hive是一个构建在hadoop基础设施上的数据仓库，通过hive可以使用hql语言查询存放在hdfs上的数据。hql是一种类sql语言，这种语言最终被转化为map/reduce。虽然hive提供了sql查询功能，但是hive不能进行交互查询。（只能在hadoop上批量执行hadoop） hbase是一种key/value系统，运行在hdfs之上。和hive不一样，hbase能够在数据库上实时运行，而不是运行map/reduce任务。hive被分区为表格，表格又被进一步分割为列簇。列簇必须使用schema定义，列簇将某一类型列集合起来。每一个key/value对在hbase中被定义为一个cell，每一个key由row-key，列簇，列和时间戳。在hbase中，行是key/value映射的集合，这个映射通过row-key来唯一标识。 特点 hive帮助熟悉sql的人运行map/reduce任务。因为它是jdbc兼容的，同时它也能够和现存的sql工具整合一起。运行hive查询会花费很长时间，因为它会默认遍历表中所有数据。虽然有这样的缺点，一次遍历的数据量可以通过hive的分区机制来控制。分区允许在数据集上运行过滤查询，这些数据集存储在不同的文件夹内，查询的时候只遍历指定文件夹（分区）中的数据。 hbase通过存储key/value来工作。它支持四种主要的操作，增加或者更新行，查看一个范围内的cell，获取指定的行，删除指定的行、列或者列的版本。版本信息用来获取历史数据（每一行的历史数据可以被删除，然后通过hbase compaction就可以释放空间）。虽然hbase包括表格，但是schema仅仅被表格和列簇所要求，列不需要schema。hbase的表格包括增加／计数功能。 限制 hive目前不支持更新操作。另外，由于hive在hadoop上运行批量操作，它需要花费很长的时间，通常是几分钟到几个小时才可以获取到查询的结果。Hive必须提供预先定义好的schema将文件和目录映射到列，并且Hive与ACID不兼容。 HBase查询是通过特定的语言来编写的，这种语言需要重新学习。类SQL的功能可以通过Apache Phonenix实现，但这是以必须提供schema为代价的。另外，Hbase也并不是兼容所有的ACID特性，虽然它支持某些特性。最后但不是最重要的–为了运行Hbase，zookeeper是必须的，zookeeper是一个用来进行分布式协调的服务，这些服务包括配置服务，维护元信息和命名空间服务。 应用场景 Hive适合用来对一段时间内的数据进行分析查询，例如，用来计算趋势或者网站的日志。Hive不应该用来进行实时的查询。因为它需要很长时间才可以返回结果。 Hbase非常适合用来进行大数据的实时查询。Facebook用Hbase进行消息和实时的分析。它也可以用来统计Facebook的连接数。 总结Hive和Hbase是两种基于Hadoop的不同技术–Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"二叉树的非递归遍历","slug":"binary-tree-non-recursive-traversal","date":"2017-02-01T05:19:19.000Z","updated":"2017-02-02T16:01:15.000Z","comments":true,"path":"2017/02/01/binary-tree-non-recursive-traversal/","link":"","permalink":"http://yoursite.com/2017/02/01/binary-tree-non-recursive-traversal/","excerpt":"二叉树定义123456789class TreeNode &#123;public int val; TreeNode *left, *right; TreeNode(int val) &#123; this -&gt; val = val; this -&gt; left = this -&gt; right = NULL; &#125;&#125; 前序遍历根据前序遍历访问的顺序，优先访问根结点，然后再分别访问左孩子和右孩子。即对于任一结点，其可看做是根结点，因此可以直接访问，访问完之后，若其左孩子不为空，按相同规则访问它的左子树；当访问其左子树时，再访问它的右子树。因此其处理过程如下：","text":"二叉树定义123456789class TreeNode &#123;public int val; TreeNode *left, *right; TreeNode(int val) &#123; this -&gt; val = val; this -&gt; left = this -&gt; right = NULL; &#125;&#125; 前序遍历根据前序遍历访问的顺序，优先访问根结点，然后再分别访问左孩子和右孩子。即对于任一结点，其可看做是根结点，因此可以直接访问，访问完之后，若其左孩子不为空，按相同规则访问它的左子树；当访问其左子树时，再访问它的右子树。因此其处理过程如下： 对于任一结点P： 访问结点P，并将结点P入栈; 判断结点P的左孩子是否为空，若为空，则取栈顶结点并进行出栈操作，并将栈顶结点的右孩子置为当前的结点P，循环至1; 若不为空，则将P的左孩子置为当前的结点P; 直到P为NULL并且栈为空，则遍历结束。 12345678910111213141516171819202122232425262728293031323334void preorder1(TreeNode *root) &#123; stack&lt;TreeNode *&gt; s; if (root == NULL) return; s.push(root); while (!s.empty()) &#123; root = s.top(); s.pop(); /* 访问根结点 */ cout &lt;&lt; root -&gt; val &lt;&lt; endl; /* 右子结点先入栈 */ if (root -&gt; right) s.push(root -&gt; right); /* 左子结点后进栈 */ if (root -&gt; left) s.push(root -&gt; left); &#125;&#125;void preorder2(TreeNode *root) &#123; stack&lt;TreeNode *&gt; s; while (root != NULL || !s.empty()) &#123; while (root != NULL) &#123; cout &lt;&lt; root -&gt; val &lt;&lt; endl; s.push(root); root = root -&gt; left; &#125; if (!s.empty()) &#123; root = s.top(); s.pop(); root = root -&gt; right; &#125; &#125;&#125; 中序遍历根据中序遍历的顺序，对于任一结点，优先访问其左孩子，而左孩子结点又可以看做一根结点，然后继续访问其左孩子结点，直到遇到左孩子结点为空的结点才进行访问，然后按相同的规则访问其右子树。因此其处理过程如下： 对于任一结点P， 若其左孩子不为空，则将P入栈并将P的左孩子置为当前的P，然后对当前结点P再进行相同的处理； 若其左孩子为空，则取栈顶元素并进行出栈操作，访问该栈顶结点，然后将当前的P置为栈顶结点的右孩子; 直到P为NULL并且栈为空则遍历结束。 123456789101112131415void inorder(TreeNode *root) &#123; stack&lt;TreeNode *&gt; s; while (!s.empty() || root != NULL) &#123; /* 首先根结点入栈，然后左子结点入栈 */ while (root != NULL) &#123; s.push(root); root = root -&gt; left; &#125; if (root != NULL) &#123; root = s.top(); s.pop(); cout &lt;&lt; root -&gt; val &lt;&lt; endl; root = root -&gt; right; &#125;&#125; 后序遍历后序遍历的非递归实现是三种遍历方式中最难的一种。因为在后序遍历中，要保证左孩子和右孩子都已被访问并且左孩子在右孩子前访问才能访问根结点，这就为流程的控制带来了难题。 第一种思路：对于任一结点P，将其入栈，然后沿其左子树一直往下搜索，直到搜索到没有左孩子的结点，此时该结点出现在栈顶，但是此时不能将其出栈并访问，因此其右孩子还为被访问。所以接下来按照相同的规则对其右子树进行相同的处理，当访问完其右孩子时，该结点又出现在栈顶，此时可以将其出栈并访问。这样就保证了正确的访问顺序。可以看出，在这个过程中，每个结点都两次出现在栈顶，只有在第二次出现在栈顶时，才能访问它。因此需要多设置一个变量标识该结点是否是第一次出现在栈顶。 123456789101112131415161718192021222324252627282930313233struct StackNode &#123; TreeNode *root; bool isFirst; // 增加状态变量&#125;/* visit(left) -&gt; visit(right) -&gt; root */void postorder(TreeNode *root) &#123; stack&lt;StackNode&gt; s; StackNode snode; while (!s.empty() || root != NULL) &#123; /* 有左孩子，则进栈，将root赋值为root -&gt; left */ while (root != NULL) &#123; snode.isFirst = true; snode.root = root; s.push(snode); root = root -&gt; left; &#125; if (!s.empty()) &#123; snode = s.top(); s.pop(); /* 栈顶第一次出现某结点，先pop()再push()，将root赋值为root -&gt; right */ if (snode.isFirst) &#123; snode.isFirst = false; s.push(snode); root = snode.root -&gt; right; /* 结点会出现在栈顶两次，访问结点 */ &#125; else &#123; cout &lt;&lt; snode.root -&gt; val &lt;&lt; endl; root = NULL; &#125; &#125; &#125;&#125; 第二种思路：要保证根结点在左孩子和右孩子访问之后才能访问，因此对于任一结点P，先将其入栈。如果P不存在左孩子和右孩子，则可以直接访问它；或者P存在左孩子或者右孩子，但是其左孩子和右孩子都已被访问过了，则同样可以直接访问该结点。若非上述两种情况，则将P的右孩子和左孩子依次入栈，这样就保证了每次取栈顶元素的时候，左孩子在右孩子前面被访问，左孩子和右孩子都在根结点前面被访问。 12345678910111213141516171819202122void postorder(TreeNode *root) &#123; stack&lt;TreeNode *&gt; s; TreeNode *cur, *pre = NULL; if (root == NULL) return; s.push(root); while (!s.empty()) &#123; cur = s.top(); /* cur不存在左孩子和右孩子 */ if ((cur -&gt; left == NULL &amp;&amp; cur -&gt; right == NULL) || pre != NULL &amp;&amp; (pre == cur -&gt; left || pre == cur -&gt; right)) &#123; cout &lt;&lt; cur -&gt; val &lt;&lt; endl; s.pop(); pre = cur; &#125; else &#123; if (cur -&gt; left) s.push(cur -&gt; left); if (cur -&gt; right) s.push(cur -&gt; right); &#125; &#125;&#125; 三合一非递归遍历12345678910111213141516171819202122232425262728293031323334353637383940struct StackNode1 &#123; TreeNode *root; int state; // 增加状态变量&#125;void inorder(TreeNode *root) &#123; StackNode1 snode; snode.root = root; snode.state = 0; stack&lt;StackNode1&gt; s; while (snode.root != NULL &amp;&amp; !s.empty()) &#123; while (!s.empty() &amp;&amp; (snode.root == NULL || snode.state &gt;= 3)) &#123; snode = s.top(); s.pop(); snode.state++; &#125; if (snode.root == NULL || snode.state &gt;= 3) break; /* 当前是中序遍历，前序遍历和后序遍历只需更改case的值 */ /* 前序遍历：case 1 -&gt; case 0 -&gt; case 2 */ /* 后序遍历：case 1 -&gt; case 2 -&gt; case 0 */ switch (snode.state) &#123; case 0: s.push(snode); snode.root = snode.root -&gt; left; break; case 1: cout &lt;&lt; snode.root -&gt; val &lt;&lt; endl; snode.state++; break; case 2: s.push(snode); snode.root = snode.root -&gt; right; snode.state = 0; break; default: break; &#125; &#125;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/数据结构/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"http://yoursite.com/tags/二叉树/"}]}]}